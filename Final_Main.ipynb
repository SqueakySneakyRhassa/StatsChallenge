{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our own functions\n",
    "import Functions as Func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the data and apply our data processing steps\n",
    "\n",
    "We define 'model data' as the set collectively used for training & validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Func.load_and_basic_process()\n",
    "df_X, df_Y = Func.df_data_process(df)\n",
    "\n",
    "X_model, X_test, Y_model, Y_test = train_test_split(df_X, df_Y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_model, Y_model, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of comparison we apply a simple intercept-only poisson model, and then a poisson model using our predictors, to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of PoissonRegressor on target Frequency with only the intercept\n",
      "subset               train    test\n",
      "metric                            \n",
      "D² explained        0.0000 -0.0000\n",
      "mean abs. error     0.1401  0.1406\n",
      "mean squared error  0.2391  0.2450\n"
     ]
    }
   ],
   "source": [
    "# Intercept poisson model\n",
    "\n",
    "n_samples = X_train.shape[0]  # Number of samples\n",
    "X_train_intercept = np.ones((n_samples, 1))\n",
    "\n",
    "glm_int = PoissonRegressor(alpha=1e-4, solver=\"newton-cholesky\")\n",
    "glm_int.fit(X_train_intercept, Y_train[\"Frequency\"], sample_weight=Y_train[\"Exposure\"])\n",
    "\n",
    "n_samples_test = Y_test.shape[0]  # Number of samples\n",
    "X_test_intercept = np.ones((n_samples_test, 1))\n",
    "\n",
    "# Evaluate the model\n",
    "scores = Func.score_estimator(\n",
    "    glm_int,\n",
    "    X_train_intercept,\n",
    "    X_test_intercept,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    target=\"Frequency\",\n",
    "    weights=\"Exposure\",\n",
    ")\n",
    "print(\"Evaluation of PoissonRegressor on target Frequency with only the intercept\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of PoissonRegressor on target Frequency\n",
      "subset               train    test\n",
      "metric                            \n",
      "D² explained        0.0191  0.0193\n",
      "mean abs. error     0.1378  0.1385\n",
      "mean squared error  0.2375  0.2434\n"
     ]
    }
   ],
   "source": [
    "# Next we review a poisson model using predictive features\n",
    "glm_freq = PoissonRegressor(alpha=1e-4, solver=\"newton-cholesky\")\n",
    "glm_freq.fit(X_train, Y_train[\"Frequency\"], sample_weight=Y_train[\"Exposure\"])\n",
    "\n",
    "# Model Evaluation\n",
    "scores = Func.score_estimator(\n",
    "    glm_freq,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    target=\"Frequency\",\n",
    "    weights=\"Exposure\",\n",
    ")\n",
    "print(\"Evaluation of PoissonRegressor on target Frequency\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these models available, we next can apply XGBoost as comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We change the dataframe's into efficient XGBoost Dmatrixies, which contain just the predictors, the labels (frequency) and weights (exposure)\n",
    "# Since XGBoost applies internal train-validation, we use the 'model' dataset for training\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(X_model, label=Y_model[\"Frequency\"], weight=Y_model[\"Exposure\"])\n",
    "dtest_reg = xgb.DMatrix(X_test, label=Y_test[\"Frequency\"], weight=Y_test[\"Exposure\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use very basic hyperparamters\n",
    "\n",
    "params = {\"objective\": \"reg:absoluteerror\", \"tree_method\": \"hist\"}\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.07361\tvalidation-mae:0.07423\n",
      "[1]\ttrain-mae:0.07360\tvalidation-mae:0.07423\n",
      "[2]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[3]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[4]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[5]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[6]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[7]\ttrain-mae:0.07360\tvalidation-mae:0.07421\n",
      "[8]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[9]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[10]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[11]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[12]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[13]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[14]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[15]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n",
      "[16]\ttrain-mae:0.07360\tvalidation-mae:0.07422\n"
     ]
    }
   ],
   "source": [
    "# Our initial results look promising\n",
    "\n",
    "n = 1000\n",
    "xgb_freq = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   early_stopping_rounds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0        0.074094       0.000742       0.074457      0.002993\n"
     ]
    }
   ],
   "source": [
    "# We apply a 5-fold CV on the testing data to see if there is overfitting: the results seem to hold\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "   params,\n",
    "   dtest_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics = [\"mae\"],\n",
    "   early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt another run using tuned hyperparameters which we attained from our tuning file to compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[1]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[2]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[3]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[4]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[5]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[6]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[7]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[8]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[9]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n",
      "[10]\ttrain-mae:0.07363\tvalidation-mae:0.07422\n"
     ]
    }
   ],
   "source": [
    "n = 597\n",
    "\n",
    "params = {\"objective\": \"reg:absoluteerror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.075,\n",
    "            \"max_depth\": 1,\n",
    "            \"subsample\": 0.5659,\n",
    "            \"colsample_bytree\": 0.8832,\n",
    "            \"max_delta_step\": 0.3655,\n",
    "            \"min_child_weight\": 8.5903,\n",
    "            \"gamma\": 2.798}\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n",
    "\n",
    "xgb_freq = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   early_stopping_rounds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0        0.074224       0.000731       0.074225      0.002932\n"
     ]
    }
   ],
   "source": [
    "# What we see are extremely minor differences. This confirms to us that the default parameters are effective for our purposes.\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "   params,\n",
    "   dtest_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics = [\"mae\"],\n",
    "   early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TabNetImp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
